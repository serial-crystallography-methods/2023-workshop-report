# Modelling photons for serial crystallography

As serial crystallography is maturing as a technique complementary to single crystal diffraction, we examine how the physical model of the diffraction experiment is evolving to match. Although serial crystallography experiments allow us to probe time series and radiation-sensitive samples without some of the limitations of single crystal experiments, analyzing still diffraction patterns is comparatively challenging and far from a solved problem. Specifically, predicting still diffraction patterns, modeling and fitting or integrating Bragg peaks, accounting for partiality, and understanding how to ascertain the success of these efforts are key active areas of research. This lack of thorough understanding of diffraction events in the context of serial crystallography manifests in a gap between R-factors for still and rotational diffraction data that thus far has not been bridged. 

Predicting still diffraction patterns has been a major goal of a handful of recent efforts in crystallographic data processing for both XFEL and synchrotron use cases. A number of physical parameters, such as crystal mosaicity, background, and potentially diffuse scattering, need to be accurately modeled to achieve this goal. The difficulty of this problem can be illustrated by considering a single diffraction pattern: each structure factor amplitude has contributions from every atom, so small perturbations anywhere in the crystal produce changes in the measured intensity. Errors anywhere in the physical model similarly affect all predicted intensities and therefore all refined structure factors. Additionally, because we are usually extracting signals against a strong and noisy background, it is difficult to distinguish between errors in our model and expected levels of noise. This raises the general question of which parameters are not being modeled adequately and what work needs to be done to resolve this. 

It is well known that accurate estimates of mosaicity are critical to accurate predictions of pixel intensities. Mosaicity is a complicated feature to model, as there are many different ways to parameterize variation within a crystal lattice, and there may be additional heterogeneity across identically prepared microcrystals in a batch. Most parameterizations in current use include, at minimum, the average size of a mosaic block (within which unit cells are presumed to be perfectly aligned) and the angular spread of the orientations of these blocks. There are many other possible quantities to model and refine, including the extension of any scalar parameter to a matrix, as may be important for an anisotropic crystal. The ultimate goal is to have a complete enough model that refinement can converge to a solution very close to ground truth, while avoiding overparameterizing. To achieve that, an empirical model might be the best path forward insofar as it allows us to account for physics that is not yet understood. However, theoretical approaches can be used to isolate different factors and to understand a ground truth.  

Fortunately, there are many ideas on how to start solving some of these issues. To try to understand small differences in diffraction patterns, pairs or sets of related datasets, which will naturally have subtle differences, could be collected to attempt to systematically account for these differences. In general, measuring the same structure factors across a large set of images provides a valuable set of constraints with which model parameters can be fit per image. This approach could be very sensitive to small changes and could also be an opportunity for applying artificial intelligence to help determine trends. Additionally, simulation of structure factor intensities for anomalous pairs could also be a fruitful path as there will be small differences that are on the same scale as time-resolved differences.  

Spots recorded during still diffraction events are ‘partial’ Bragg peaks, meaning only a fraction of the full peak is measured on a given image. With no prior crystal orientation information, this can make scaling and merging the integrated intensities a challenge. Many post-refinement techniques, which rely on accurate indexing as a means of properly scaling and combining partial reflections, have been adapted from conventional crystallography to address this problem. The goal is essentially to determine the exact fraction of the reflection that has been recorded to implement a proper scale factor to retrieve the maximum, but with the difficulty of predicting still diffraction patterns and the existence of real, physical differences between microcrystals, this is not a simple task. Typically an iterative approach is used to find the ‘best’ definition of the geometry of the diffraction event to most accurately define the partial reflection, but specific approaches vary between algorithms. For XFEL experiments, shot to shot differences in several characteristics of the X-ray pulse are key parameters to correct for and require utilizing the X-ray spectra directly. 

Interestingly, accounting for partiality only seems to improve data quality some of the time, for reasons that are not well understood. Generally, post-refinement techniques help to improve already high quality datasets, but poorer quality datasets are not rescued by its implementation. It is possible that these ‘bad’ datasets have other systematic issues, such as incorrect geometry or detector gain problems. This does raise the question of the benefit of investing research time into further development of post-refinement algorithms when perhaps other aspects, that might be more successful at drastically improving data quality, could be worked on instead. A first major goal is figuring out a robust figure of merit. Right now, $R_{split}$, which compares the final intensities from each half of the dataset and checks their level of agreement, is typically used. Some other good choices include $CC_{1/2}$, anomalous peak height, a Z-score, and success of autobuilding programs as these are all sensitive metrics of reflection data statistics. Contextualizing a physical model to the partiality problem is also needed. Alternatively, using a beam with a wide bandwidth could be a useful experimental change to help deal with partiality as more complete Bragg peaks will be recorded due to a larger portion of reciprocal space being excited with each diffraction event. 

Finding a way to utilize the information captured directly from the pixels could also be a route to a more successful partiality solution. A relatively new merging algorithm, Careless, is working towards that, as it utilizes the unmerged structure factors, effectively the pixel values, as the intermediate to producing an electron density. This varies from the typical pipeline which typically compresses the data significantly. The goal of Careless is to use integrated intensities and experimental metadata to estimate the merged structure factors. It utilizes expectations from a random atom model as a prior and tries to achieve the maximum likelihood for the observed data, and the model is trained for each dataset. Partiality is then handled on a per image basis and tries to determine how far off the experimentally refined geometry is from the ideal.  

Modeling peaks for pixel integration is another major task for serial crystallography. The fundamental question comes down to determining if ray tracing or empirical information is a better model for defining reflections. Ideally, any model that works would be valid, but the ultimate goal would be to be able to fit boxes around every Bragg spot and to predict the shape of the spots. It seems likely that an empirical model might be the most promising as it will be able to account for physics that is currently unknown, but a theoretical model would help define a ground truth. In practice, both CrystFEL and DIALS utilize different models for defining peaks. The CrystFEL model uses annular rings around the reflection to define the signal and the local background for corrections. This approach can include more background noise, but overall this can help with robustness. Additionally, this has benefits for when reflections only span one pixel on the detector, which can be common for biological samples on pixel array detectors. The DIALS model uses profile fitting of the reflection, which has roots in conventional crystallography. The ultimate goal is to fit a gaussian profile to the reflection, and typically works best when the reflection spans many detector pixels. It seems plausible that both models can be useful depending on the data quality and the general shape and size of the peaks. 
